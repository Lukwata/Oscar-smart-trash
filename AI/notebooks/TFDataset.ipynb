{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset API (TensorFlow 1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.data.Dataset là API chuẩn của TensorFlow, hỗ trợ việc đọc và tiền xử lý dữ liệu (input pipeline). API này không sử dụng cơ chế Queue tổng quát như trước mà được implement bằng C, optimize cho việc đọc dữ liệu nên có perfomance cao hơn.\n",
    "\n",
    "Lợi ích của Dataset API:\n",
    "- Có các hàm high-level (batching, prefetching, ...), input pipeline code ngắn đi rất nhiều so với cơ chế Queue.\n",
    "- Hỗ trợ tạo data từ nhiều nguồn: TFRecord, từ Numpy array, từ Tensor hằng số, etc.\n",
    "- Hỗ trợ `Estimator` API: Dataset được tạo và nằm trong `input_fn`.\n",
    "- Được thiết kế tổng quát, có thể áp dụng cho nhiều loại dữ liệu: hình ảnh, text, âm thanh, etc.\n",
    "- Tiền xử lý có thể được chia làm nhiều hàm nhỏ: khả năng reuse cao."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API này bao gồm 2 thành phần chính:\n",
    "- tf.data.Dataset: object chứa tập dữ liệu.\n",
    "- tf.data.Iterator: object dùng để duyệt và lấy dữ liệu từ dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.data.Dataset`: chứa một chuỗi các element, mỗi element là một bộ Tensor (được gọi là component). \n",
    "Vd, trong bài toán image classification thì 1 element gồm 2 Tensor (image, label).\n",
    "\n",
    "Các element trong Dataset cần có cấu trúc tương tự nhau: *tf.DType* và *tf.TensorShape* của các component tương ứng của element phải giống nhau.\n",
    "Để coi cấu trúc của Dataset, ta dùng:\n",
    "- Dataset.output_types: coi type của các component của 1 element.\n",
    "- Dataset.output_shapes: coi shape của các component của 1 element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect type and shape of components\n",
    "# Modified from: https://www.tensorflow.org/programmers_guide/datasets\n",
    "\n",
    "# Each element consists of only 1 Tensor\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\n",
    "print(dataset1.output_types)  # ==> \"tf.float32\"\n",
    "print(dataset1.output_shapes)  # ==> \"(10,)\"\n",
    "\n",
    "# 2 Tensors\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "   (tf.random_uniform([4]),\n",
    "    tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
    "print(dataset2.output_types)  # ==> \"(tf.float32, tf.int32)\"\n",
    "print(dataset2.output_shapes)  # ==> \"((), (100,))\"\n",
    "\n",
    "# Each element is a tuple of Tensors\n",
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "print(dataset3.output_types)  # ==> (tf.float32, (tf.float32, tf.int32))\n",
    "print(dataset3.output_shapes)  # ==> \"(10, ((), (100,)))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset được tạo bằng 2 cách:\n",
    "1. Tạo từ Tensor: tạo Dataset từ 1 list các Tensor hoặc đọc từ TFRecord.\n",
    "2. Tạo từ Dataset: `transform` Dataset, thường dùng để preprocess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để lấy element từ Dataset đã tạo, ta sử dụng `tf.data.Iterator`.\n",
    "Có 4 loại Iterator, từ đơn giản đến phức tạp:\n",
    "1. `one-shot`: duyệt qua dữ liệu theo thứ tự, Dataset phải được tạo trước, không thể truyền tham số vào để thay đổi cách duyệt được.\n",
    "2. `initializable`: ta phải khởi tạo Iterator trước khi lấy element ra (có thể truyền tham số vào *feed_dict* để cung cấp giá trị cho `place_holder` mà Dataset dùng). Lưu ý: chỉ truyền được 1 lần khi khởi tạo Iterator nên *place_holder* này không thể lưu data (vd có thể dùng để lưu tên file mà Dataset sẽ đọc).\n",
    "3. `reinitializable`: iterator này có thể được reinit nhiều lần, mỗi lần đọc từ 1 Dataset khác nhau (dùng cùng 1 iterator, build ra graph model và chạy trên tập train/val).\n",
    "4. `feedable`: một kiểu meta-iterator, giúp chọn iterator muốn dùng (mỗi iterator là của 1 Dataset khác nhau) mà không cần reinit lại iterator.\n",
    "\n",
    "**Lưu ý: v1.4 thì TF khuyến cáo chỉ nên dùng `one-shot` iterator với API Estimator.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. One-shot iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-shot iterator example\n",
    "# Modified from: https://www.tensorflow.org/programmers_guide/datasets\n",
    "\n",
    "dataset = tf.data.Dataset.range(100)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "for i in range(100):\n",
    "    value = sess.run(next_element)\n",
    "    assert i == value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initializable iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializable iterator example\n",
    "# Modified from: https://www.tensorflow.org/programmers_guide/datasets\n",
    "\n",
    "max_value = tf.placeholder(tf.int64, shape=[])\n",
    "dataset = tf.data.Dataset.range(max_value)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Initialize an iterator over a dataset with 10 elements.\n",
    "sess.run(iterator.initializer, feed_dict={max_value: 10})\n",
    "for i in range(10):\n",
    "    value = sess.run(next_element)\n",
    "    assert i == value\n",
    "\n",
    "# Initialize the same iterator over a dataset with 100 elements.\n",
    "sess.run(iterator.initializer, feed_dict={max_value: 100})\n",
    "for i in range(100):\n",
    "    value = sess.run(next_element)\n",
    "    assert i == value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reinitializable iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitializable iterator example\n",
    "# Modified from: https://www.tensorflow.org/programmers_guide/datasets\n",
    "\n",
    "# Define training and validation datasets with the same structure.\n",
    "training_dataset = tf.data.Dataset.range(100).map(\n",
    "    lambda x: x + tf.random_uniform([], -10, 10, tf.int64))\n",
    "validation_dataset = tf.data.Dataset.range(50)\n",
    "\n",
    "# A reinitializable iterator is defined by its structure. We could use the\n",
    "# `output_types` and `output_shapes` properties of either `training_dataset`\n",
    "# or `validation_dataset` here, because they are compatible.\n",
    "iterator = Iterator.from_structure(training_dataset.output_types,\n",
    "                                   training_dataset.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n",
    "validation_init_op = iterator.make_initializer(validation_dataset)\n",
    "\n",
    "# Run 20 epochs in which the training dataset is traversed, followed by the\n",
    "# validation dataset.\n",
    "for _ in range(20):\n",
    "    # Initialize an iterator over the training dataset.\n",
    "    sess.run(training_init_op)\n",
    "    for _ in range(100):\n",
    "        sess.run(next_element)\n",
    "\n",
    "    # Initialize an iterator over the validation dataset.\n",
    "    sess.run(validation_init_op)\n",
    "        for _ in range(50):\n",
    "    sess.run(next_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feedable iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedable iterator example\n",
    "# Modified from: https://www.tensorflow.org/programmers_guide/datasets\n",
    "\n",
    "# Define training and validation datasets with the same structure.\n",
    "training_dataset = tf.data.Dataset.range(100).map(\n",
    "    lambda x: x + tf.random_uniform([], -10, 10, tf.int64)).repeat()\n",
    "validation_dataset = tf.data.Dataset.range(50)\n",
    "\n",
    "# A feedable iterator is defined by a handle placeholder and its structure. We\n",
    "# could use the `output_types` and `output_shapes` properties of either\n",
    "# `training_dataset` or `validation_dataset` here, because they have\n",
    "# identical structure.\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(\n",
    "    handle, training_dataset.output_types, training_dataset.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# You can use feedable iterators with a variety of different kinds of iterator\n",
    "# (such as one-shot and initializable iterators).\n",
    "training_iterator = training_dataset.make_one_shot_iterator()\n",
    "validation_iterator = validation_dataset.make_initializable_iterator()\n",
    "\n",
    "# The `Iterator.string_handle()` method returns a tensor that can be evaluated\n",
    "# and used to feed the `handle` placeholder.\n",
    "training_handle = sess.run(training_iterator.string_handle())\n",
    "validation_handle = sess.run(validation_iterator.string_handle())\n",
    "\n",
    "# Loop forever, alternating between training and validation.\n",
    "while True:\n",
    "    # Run 200 steps using the training dataset. Note that the training dataset is\n",
    "    # infinite, and we resume from where we left off in the previous `while` loop\n",
    "    # iteration.\n",
    "    for _ in range(200):\n",
    "        sess.run(next_element, feed_dict={handle: training_handle})\n",
    "\n",
    "    # Run one pass over the validation dataset.\n",
    "    sess.run(validation_iterator.initializer)\n",
    "    for _ in range(50):\n",
    "        sess.run(next_element, feed_dict={handle: validation_handle})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full pipeline với Estimator API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để dùng Dataset API với Estimator API, ta implement `input_fn` bằng cách dùng Dataset, iterator và return lại Tensor lấy từ Dataset.\n",
    "\n",
    "Ví dụ để tạo input pipeline đọc dữ liệu từ TFRecord cho bài toán image classification:\n",
    "1. `input_fn` nhận file_patterns và lấy hết tất cả file TFRecord có pattern này.\n",
    "2. Implement hàm `parser` để parse proto buff message với input là 1 record khi đọc được từ TFRecord. Hàm này return 2 Tensor là image đã decode và label.\n",
    "3. Implement hàm `preprocess`, nhận 2 Tensor của hàm trên và build graph để tiền xử lý ảnh. Hàm này return theo định dạng của `input_fn` mà Estimator yêu cầu là features là 1 dict và labels là 1 Tensor.\n",
    "4. Tạo `tf.data.TFRecordDataset` từ list tên file TFRecord. Dataset này sẽ đọc từ TFRecord và trả về string là protocol buffer message được lưu trong đó (cơ chế tự advance phần tử tiếp theo, advance qua file tiếp theo hay buffer các phần tử đã có sẵn, không cần tự cài đặt).\n",
    "5. Gọi hàm `parser` để parse record từ Dataset trên bằng cách dùng hàm `map` (apply 1 hàm cho từng phần tử).\n",
    "6. Gọi hàm `preprocess` nhận 2 Tensor từ hàm trên và tiền xử lý dữ liệu cũng bằng hàm `map`.\n",
    "7. Thực hiện shuffle.\n",
    "8. Repeat để lặp train vô hạn (thêm tham số là số lần lặp để hữu hạn). Ta sẽ truyền num_steps vào Estimator để dừng việc train/eval.\n",
    "9. Gọi prefetch để tạo nhiều thread đọc dữ liệu.\n",
    "10. Tạo one-shot iterator và return kết quả."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(file_patterns, is_training, batch_size):\n",
    "    \"\"\"Input function for Resnet Estimator.\"\"\"\n",
    "    filenames = []\n",
    "    for pattern in file_patterns.split(\",\"):\n",
    "        filenames.extend(tf.gfile.Glob(pattern))\n",
    "\n",
    "    def parser(record):\n",
    "        \"\"\"Parse tf.Example protobuf.\"\"\"\n",
    "        keys_to_features = {\n",
    "            \"image/data\": tf.FixedLenFeature((), tf.string),\n",
    "            \"label\": tf.FixedLenFeature((), tf.int64)\n",
    "        }\n",
    "        parsed = tf.parse_single_example(record, features=keys_to_features)\n",
    "\n",
    "        image = tf.image.decode_image(parsed[\"image/data\"], channels=3)\n",
    "        label = parsed[\"label\"]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def preprocess(image, label):\n",
    "        \"\"\"Preprocess image for Resnet.\"\"\"\n",
    "        image_size = resnet_v1.resnet_v1_101.default_image_size\n",
    "        image = vgg_preprocessing.preprocess_image(image, image_size,\n",
    "                                                   image_size,\n",
    "                                                   is_training=is_training)\n",
    "        label = tf.to_int32(label)\n",
    "        return {\"image\": image}, label\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Create the dataset\n",
    "        dataset = tf.data.TFRecordDataset(filenames, buffer_size=256 * 2 ** 20)\n",
    "        dataset = dataset.map(parser, num_parallel_calls=batch_size * 10)\n",
    "        dataset = dataset.map(preprocess,\n",
    "                              num_parallel_calls=batch_size * 10)\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "        dataset = dataset.repeat()\n",
    "        dataset = dataset.batch(batch_size=batch_size)\n",
    "        dataset = dataset.prefetch(batch_size)\n",
    "\n",
    "        # Create iterator for the dataset\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        features, labels = iterator.get_next()\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đọc từng dòng trong file text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset API hỗ trợ đọc dữ liệu dạng text hoặc CSV (mỗi dòng là 1 element).\n",
    "\n",
    "Ví dụ để tạo input pipeline đọc từ file text:\n",
    "1. Tạo dataset lưu tên file.\n",
    "2. Tạo TextLineDataset từ từng file của dataset trên.\n",
    "3. Skip dòng đầu.\n",
    "4. Filter những dòng comment.\n",
    "5. Flatten các element của tất cả file (cho cùng cấp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read line-by-line from text file example\n",
    "# Modified from: https://www.tensorflow.org/programmers_guide/datasets\n",
    "\n",
    "filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\"]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "\n",
    "# Use `Dataset.flat_map()` to transform each file as a separate nested dataset,\n",
    "# and then concatenate their contents sequentially into a single \"flat\" dataset.\n",
    "# * Skip the first line (header row).\n",
    "# * Filter out lines beginning with \"#\" (comments).\n",
    "dataset = dataset.flat_map(\n",
    "    lambda filename: (\n",
    "        tf.data.TextLineDataset(filename)\n",
    "        .skip(1)\n",
    "        .filter(lambda line: tf.not_equal(tf.substr(line, 0, 1), \"#\"))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
