{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving (TensorFlow 1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Một cách đơn giản, để serve 1 model TF, ta cần:\n",
    "    1. Save model đã train (load từ ckpt) thành SavedModel object.\n",
    "    2. Build TF Serving `tensorflow_model_server` (binary này dùng để chạy server serving) bằng bazel hoặc install pip-package.\n",
    "    3. Chạy server với đường dẫn đến SavedModel (có thể enable batching).\n",
    "    4. Khi có model mới, copy SavedModel mới vào thư mục trên => server sẽ tự động reload model mới."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"serving_architecture.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF Serving bao gồm các thành phần sau:\n",
    "    - **Servable**: là 1 model hoặc một phần model (có thể không phải model TensorFlow). Thông thường thì servable sẽ bao gồm SavedModelBundle (chứa Session) và look-up table. Mỗi servable sẽ có 1 version (số int tăng dần) và nhiều version có thể được chạy cùng lúc (client có thể request 1 version cụ thể). 1 list các version của 1 servable được gọi là **servable stream**.\n",
    "    - **Loader**: thực hiện load, unload servable. Loader cơ bản nhất nhận đường dẫn đến 1 SavedModel và load, unload model. Loader được dùng để thống nhất API load và unload servable bất kỳ.\n",
    "    - **Source**: với mỗi servable stream, source tạo ra 1 loader tương ứng để load, unload servable. Source là thành phần poll 1 folder để xem khi nào có version mới.\n",
    "    - **Manager**: quản lý life-cycle của servable: khi nào cần load, unload hoặc serve. Khi có version mới, source tạo loader cho version này, loader đánh dấu model là ready to load (gọi là **aspired version**) và thông báo cho manager. Manager sẽ quyết định có load và serve version mới này hay không và có unload version cũ hay không. Hiện tại có 2 **version policy** được implement:\n",
    "        - Availability preserving: load version mới trước rồi mới unload version cũ => luôn có 1 version được chạy nhưng x2 resource.\n",
    "        - Resource preserving: unload version cũ rồi load version mới."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF Serving đã implement những thành phần này để ta có thể serve 1 model với những tính năng cơ bản. Để serve những model phức tạp hoặc cần quy tắc serving đặc biệt, ta có thể extend thêm:\n",
    "    - Loader: implement loader đọc từ cloud, từ database, ...\n",
    "    - Source: lưu state của các servable để share lẫn nhau.\n",
    "    - Batching: cơ chế batching phức tạp hơn hoặc optimize hơn cho model cần serve.\n",
    "    - Version policy: cơ chế load, unload phức tạp hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Serving non-Estimator model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export SavedModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SavedModel là định dạng mới của TF để lưu model (thay cho *SessionBundle*). SavedModel lưu các thành phần sau:\n",
    "    - Một list các graph (mỗi graph có một list các tag để phân biệt).\n",
    "    - Giá trị của variable để dùng chung cho tất cả các graph (ta có thể strip device ra khỏi graph).\n",
    "    - Input, output của những API để serving và loại API đó (gọi là **Signature**, khi lưu dưới dạng protobuf message thì được gọi là **SignatureDefs**).\n",
    "    - Các asset như dictionary.\n",
    "    - Các extra-asset khác mà user muốn lưu kèm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF cung cấp SavedModelBuilder để export SavedModel:\n",
    "    - Meta graph đầu tiên của SavedModel phải được lưu kèm với variable.\n",
    "    - Những meta graph tiếp theo thì không lưu variable (dùng chung).\n",
    "    - Mỗi meta graph phải đính kèm một số tag (như training, serving, gpu, ...) để phân biệt chúng và cho biết chức năng từng graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = \"\"\n",
    "\n",
    "builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    # Build first meta graph\n",
    "    # ...\n",
    "\n",
    "    # Add to SavedModel\n",
    "    builder.add_meta_graph_and_variables(sess,\n",
    "                                         [tf.saved_model.tag_constants.TRAINING],\n",
    "                                         signature_def_map=foo_signatures,\n",
    "                                         assets_collection=foo_assets)\n",
    "\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    # Build another meta graph\n",
    "    builder.add_meta_graph([\"bar-tag\", \"baz-tag\"])\n",
    "\n",
    "builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF định nghĩa sẵn một số tag và tên của SignatureDefs thường dùng:\n",
    "    - `tf.saved_model.tag_constants`: SERVING, TRAINING, GPU, TPU.\n",
    "    - `tf.saved_model.signature_constants`: CLASSIFY_INPUTS, CLASSIFY_METHOD_NAME, CLASSIFY_OUTPUT_CLASSES, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mỗi meta-graph trong 1 SavedModel bao gồm nhiều **Signature**. Mỗi meta-graph sẽ có một mapping (dict *signature_def_map*) từ tên của signature (client dùng tên này để request) đến signature đó. Luôn có 1 signature default với tên *serving_default* được sử dụng khi client không request tên 1 signature cụ thể nào.\n",
    "\n",
    "\n",
    "- TF định nghĩa sẵn 3 loại signature kèm với tên input và output:\n",
    "    1. Classify (`CLASSIFY_METHOD_NAME`): nhận *input* (`CLASSIFY_INPUTS`) output *classes* (`CLASSIFY_OUTPUT_CLASSES`) và *scores* (`CLASSIFY_OUTPUT_SCORES`) cho bài toán classification.\n",
    "    2. Regress (`REGRESS_METHOD_NAME`): nhận *input* (`REGRESS_INPUTS`) output giá trị kết quả (`REGRESS_OUTPUTS`) cho bài toán regression.\n",
    "    3. Predict (`PREDICT_METHOD_NAME`): tổng quát nhất, nhận input (tên user tự đặt) và output kết quả (tên user tự đặt).\n",
    "\n",
    "    \n",
    "- TF cung cấp hàm để build SignatureDefs: `tf.saved_model.signature_def_utils.build_signature_def`:\n",
    "    - inputs: mapping từ tên input đến TensorInfo object (dùng `tf.saved_model.utils_impl.build_tensor_info` để build).\n",
    "    - outputs: tương tự inputs.\n",
    "    - method_name: loại API (1 trong 3 loại trên)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a classify signature:\n",
    "\n",
    "signature_def: {\n",
    "  key  : \"my_classification_signature\"  # Signature name\n",
    "  value: {\n",
    "    inputs: {\n",
    "      key  : \"inputs\"\n",
    "      value: {\n",
    "        name: \"tf_example:0\"\n",
    "        dtype: DT_STRING\n",
    "        tensor_shape: ...\n",
    "      }\n",
    "    }\n",
    "    outputs: {\n",
    "      key  : \"classes\"  # Name of an output\n",
    "      value: {\n",
    "        name: \"index_to_string:0\"  # Mapping to a Tensor in the graph\n",
    "        dtype: DT_STRING\n",
    "        tensor_shape: ...\n",
    "      }\n",
    "    }\n",
    "    outputs: {\n",
    "      key  : \"scores\"  # Name of another output\n",
    "      value: {\n",
    "        name: \"TopKV2:0\"\n",
    "        dtype: DT_FLOAT\n",
    "        tensor_shape: ...\n",
    "      }\n",
    "    }\n",
    "    method_name: \"tensorflow/serving/classify\"  # Signature type\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- Classify và Regress luôn phải nhận input là string chứa deserialized tf.Example protobuf nên model khi serve phải parse example này ra Tensor => không được flexible như Predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ModelServer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Để serve model vừa export:\n",
    "    1. Build `tensorflow_model_server` bằng bazel (có thể build GPU enabled) hoặc install package `tensorflow-model-server`.\n",
    "    2. Run server: `tensorflow_model_server --model_name=<model-name>  --port=9000 --model_base_path=<path-to-SavedModel>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sau khi build TF Serving bằng bazel, ta có thể copy những file proto được auto generated để build request phía client (gồm các file `classification_pb2.py`, `get_model_metadata_pb2.py`, `inference_pb2.py`, `input_pb2.py`, `model_pb2.py`, `predict_pb2.py`, `prediction_service_pb2.py`, `regression_pb2.py`).\n",
    "- Tuy nhiên, hiện tại client request phải gửi lên protobuf message của Tensor nên vẫn phải dùng TF để tạo cho dễ. Ta có thể copy hàm `make_tensor_proto` và các dependency ra dùng riêng để phía client không phải install Tf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connection\n",
    "channel = implementations.insecure_channel(host, int(port))\n",
    "stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n",
    "\n",
    "# Build request\n",
    "request = predict_pb2.PredictRequest()\n",
    "request.model_spec.name = \"<model-name>\"\n",
    "request.model_spec.signature_name = \"<signature-to-request>\"\n",
    "request.inputs[\"image\"].CopyFrom(tf.contrib.util.make_tensor_proto([data], dtype=tf.string))\n",
    "\n",
    "# Make request\n",
    "result = stub.Predict(request, 10.0)  # 10 seconds timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Serving Estimator model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input function cho model lúc serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Estimator hỗ trợ export model đã train thành SavedModel bằng hàm `estimator.export_savedmodel`. Ta cần cung cấp `serving_input_receiver_fn` để tiền xử lý dữ liệu nhận được trong lúc serving và trả về *feature* cho *model_fn*.\n",
    "\n",
    "- Estimator chỉ hỗ trợ export SavedModel với signature thuộc loại **Predict**. Vì vậy `serving_input_receiver_fn` sẽ nhận Tensor (thay vì tf.Example). Hàm này cần trả về `ServingInputReceiver` - một  cấu trúc dữ liệu lưu mapping giữa Tensor nhận được từ Serving đến input của model_fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image_string_tensor(encoded_image_string_tensor):\n",
    "    \"\"\"Decode an a string of bytes represent an image Tensor.\"\"\"\n",
    "    image_tensor = tf.image.decode_image(encoded_image_string_tensor,\n",
    "                                         channels=3)\n",
    "    image_tensor.set_shape((None, None, 3))\n",
    "    return image_tensor\n",
    "\n",
    "def serving_input_receiver_fn():\n",
    "    \"\"\"Parse and preprocess data for serving model.\"\"\"\n",
    "    encoded_image_string_tensor = tf.placeholder(\n",
    "        dtype=tf.string, shape=[None], name='encoded_image_string_tensor')\n",
    "    receiver_tensors = {\n",
    "        'image': encoded_image_string_tensor\n",
    "    }\n",
    "    images = tf.map_fn(decode_image_string_tensor, encoded_image_string_tensor,\n",
    "                       back_prop=False, dtype=tf.uint8)\n",
    "    images = tf.map_fn(lambda image: preprocess_image(image, is_training=False),\n",
    "                       images, back_prop=False, dtype=tf.float32)\n",
    "\n",
    "    features = {\n",
    "        'image': images\n",
    "    }\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ở ví dụ trên, Serving sẽ gửi cho ta 1 Tensor có tên là **image** thuộc dạng string (lưu JPEG encoded string của một batch các ảnh). Sau đó ta thực hiện decode ảnh và preprocess từng ảnh (dùng `tf.map_fn` để apply 1 function song song cho 1 batch).\n",
    "\n",
    "- Cuối cùng, ta tạo và return `ServingInputReceiver` với các tham số:\n",
    "    - `receiver_tensors`: mapping giữa tên Tensor mà client sẽ gửi và placeholder lưu giá trị để return cho model_fn.\n",
    "    - `features`: dict lưu Tensor để gửi cho model_fn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: \n",
    "- Nếu không cần tiền xử lý gì đặc biệt thì ta có thể dùng hàm `build_raw_serving_input_receiver_fn` hoặc `build_parsing_serving_input_receiver_fn` để tạo serving_input_receiver_fn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Định nghĩa output sẽ serve của model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tf.estimator.EstimatorSpec` nhận tham số **export_outputs** để định nghĩa những output của model cho quá trình serving (lưu ý, khi serving thì mode của Estimator sẽ là `tf.estimator.ModeKeys.PREDICT`).\n",
    "\n",
    "- Tham số này là một dict với key là tên output trong signature và value là ExportOutput object (`ClassificationOutput`, `RegressionOutput` hoặc `PredictOutput`). Lưu ý là signature này vẫn thuộc loại **Predict**, các object output trên chỉ hỗ trợ việc tạo output cho tiện (như classification sẽ tạo 2 giá trị output *classes* và *scores*).\n",
    "\n",
    "- Nếu *export_outputs* chỉ có 1 phần tử, TF sẽ tự tạo signature *serving_default* với output tương tự phần tử ta cung cấp. Nếu không có, ta phải tự tạo signature này.\n",
    "\n",
    "- Tất cả signature chỉ khác nhau phần output. Phần input luôn giống nhau là kết quả lấy từ `serving_input_receiver_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of export_outputs when creating EsimatorSpec\n",
    "\n",
    "export_outputs = {\n",
    "    \"scores\": tf.estimator.export.PredictOutput({\n",
    "        \"logits\": logits,\n",
    "        \"classes\": classes\n",
    "    })\n",
    "}\n",
    "\n",
    "tf.estimator.EstimatorSpec(mode, predictions=predictions,\n",
    "                           loss=total_loss, train_op=train_op,\n",
    "                           eval_metric_ops=eval_metric_ops,\n",
    "                           scaffold=scaffold,\n",
    "                           export_outputs=export_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Estimator model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Với input và output được định nghĩa như trên, ta tạo Estimator với *model_dir* dẫn đến checkpoint của model đã train và gọi hàm `export_savedmodel` để tạo SavedModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    \"\"\"Parse and preprocess data for serving model.\"\"\"\n",
    "    # Parse and preprocess\n",
    "    # ...\n",
    "    \n",
    "    # Return input data\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode, params, config):\n",
    "    \"\"\"Model function for Estimator.\"\"\"\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # Build model and output for serving\n",
    "        # ...\n",
    "\n",
    "        # Create output data\n",
    "        export_outputs = {\n",
    "            \"scores\": tf.estimator.export.PredictOutput({\n",
    "                \"logits\": logits\n",
    "            })\n",
    "        }\n",
    "        \n",
    "    return tf.estimator.EstimatorSpec(mode, predictions=predictions,\n",
    "                                      loss=total_loss, train_op=train_op,\n",
    "                                      eval_metric_ops=eval_metric_ops,\n",
    "                                      scaffold=scaffold,\n",
    "                                      export_outputs=export_outputs)\n",
    "    \n",
    "    \n",
    "# Create Estimator and load weights from checkpoint\n",
    "estimator = tf.estimator.Estimator(model_fn, model_dir=checkpoint_dir)\n",
    "\n",
    "# Export SavedModel\n",
    "estimator.export_savedmodel(output_dir, serving_input_receiver_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advance: C++ API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
